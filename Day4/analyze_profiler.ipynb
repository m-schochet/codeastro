{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Profiling\n",
    "\n",
    "Code profiling is the process of dynamically analyzing the performance of your code or software, focusing on aspects like runtime, memory allocation, and function execution frequency. Its primary goal is to identify performance bottlenecks, excessive resource consumption, and areas that take too long to execute, enabling developers to optimize their code for better efficiency and speed. \n",
    "\n",
    "Here we use a simple Python profiler called \"cProfile\", which is built into the Python standard library. It can be used to assess the runtime and execution frequency of all functions utilized in a script. Let's look at an example below!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running cProfile\n",
    "\n",
    "First, create a Python script that repeatedly calls the function or class you want to speed up a few hundred times atleast (this helps average out any variations in execution time due to different input parameters or simply makes sure the most time consuming functions are reflected as such in the output file). Then, run the following command from the command line:\n",
    "\n",
    "`python -m cProfile -o profiler_output.txt profile_orbitize.py`\n",
    "\n",
    "- The `-m cProfile` will attach the cProfile profiler to this script when it runs.\n",
    "\n",
    "- The `-o profiler_output.txt` will save its output to \"profiler_output.txt\" (otherwise it dumps the output stats into stdout).\n",
    "\n",
    "_Note on parallelization: Using parallelism will make the cprofiler output more confusing than necessary. We just want to know what takes the most time,\n",
    "summed up across multiple processes. Doing this on parallelized code, each process must save its own cProfile output and you \n",
    "need to combine the outputs afterwards (using `pstats.add()`)._"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing the cProfile output\n",
    "\n",
    "After you have run the `profile_orbitize.py` function with the cProfile package, we will use the `pstats` module (part of Python standard library) to read and analyze the output of the profiler (a binary file) and identify bottlenecks in the code. If you are in Colaboratory, you will need to upload the output of the cProfile to Colaboratory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pstats\n",
    "\n",
    "mystats = pstats.Stats(\"profiler_output.txt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the top 15 functions that took the longest time to run\n",
    "\n",
    "Even in such a short script, python called millions of function! A lot of them are super quick and really not what we are after. Typically, we want to find the few functions that are bottlenecks in our code.\n",
    "\n",
    "The function below sorts the functions by which took the longest to run, and prints out only the stats of the top 15 functions that took longest to run. The columns below mean:\n",
    "  * ncalls: number of times this function was called\n",
    "  * tottime: the total execution time spent in this code NOT including calls to other functions\n",
    "  * percall: this first percall divides tottime by ncalls. The amount of time per call spent solely in this function.\n",
    "  * cumtime: the total execution time spent in this code INCLUDING calls to sub functions. Note: <u>cumtime > tottime always</u>.\n",
    "  * percall: second percall divides cumtime by ncalls\n",
    "  * filename: the name of the function being considered\n",
    "\n",
    "Both `tottime` and `cumtime` are useful. A function with a high `tottime` means we should focus on speeding up this function. A function with only a high `cumtime` means we should see what this function is calling to improve runtime.\n",
    "  * `_newton_solver()` is a function with a high tottime. We should look at the lines in that function to check out how to speed it up.\n",
    "  * `compute_model()` is a function with a low tottime but a very high `cumtime`. We should look at the functions it calls to figure out what takes up so much time.\n",
    "\n",
    "Generally, we want to put some filters on `print_stats`, becuase otherwise there will be so much printed out it is unmanageable.\n",
    "\n",
    "For more features: [pstats documentaiton](https://docs.python.org/3/library/profile.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mystats.sort_stats(\"time\").print_stats(30)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also apply multiple filters. For example let's look at the top 15 numpy functions that took the most time to run. Note that the order of the filtering matters. This command first selects every function with numpy in the name, followed by taking the top 15. Calling `print_stats(15, 'numpy')` would pick the top 15 longest runtime functions, and downselecting which has numpy in the name from those. That would give us less than 15 numpy functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mystats.sort_stats(\"time\").print_stats(\"numpy\", 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystats.sort_stats(\"time\").print_stats(15, \"numpy\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activities\n",
    "\n",
    "## Activity Part 1: Analyze the output\n",
    "\n",
    "Generate the output above on your own by running `python -m cProfile -o profiler_output.txt profile_orbitize.py` and answer the following questions by analyzing the output.\n",
    "\n",
    "1. Which function takes up the most runtime not including calls to sub-functions?\n",
    "- \n",
    "\n",
    "2. Which function takes up the most runtime including calls to sub-functions?\n",
    "\n",
    "3. Which function is called the most? Which `orbitize` function is called the most?\n",
    "\n",
    "4. If we had the magical ability to speed up one of the functions in the previous three answers by a factor of 2, which function should we speed up? What is the improvement in end-to-end runtime of the script?\n",
    "\n",
    "\n",
    "_Note: Depending on your system, some inbuilt functions might turn out to be the most time consuming ones. But here we are looking for the ones we can speed up, namely the orbitize functions._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Aug  7 14:16:18 2025    profiler_output.txt\n",
      "\n",
      "         16987339 function calls (16840184 primitive calls) in 88.256 seconds\n",
      "\n",
      "   Ordered by: internal time\n",
      "   List reduced from 9904 to 30 due to restriction <30>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "     4480   18.740    0.004   18.828    0.004 /Users/mschochet/codeastro/orbitize/orbitize/kepler.py:228(_newton_solver)\n",
      "  227/224   16.454    0.072   16.459    0.073 {built-in method _imp.create_dynamic}\n",
      "     4480   14.813    0.003   44.174    0.010 /Users/mschochet/codeastro/orbitize/orbitize/kepler.py:48(calc_orbit)\n",
      "     4480    4.209    0.001   23.973    0.005 /Users/mschochet/codeastro/orbitize/orbitize/kepler.py:143(_calc_ecc_anom)\n",
      "     4480    4.155    0.001    4.155    0.001 /Users/mschochet/codeastro/orbitize/orbitize/system.py:727(radec2seppa)\n",
      "     4480    2.995    0.001    4.617    0.001 /Users/mschochet/anaconda3/envs/codeastro/lib/python3.10/site-packages/scipy/special/_logsumexp.py:192(_logsumexp)\n",
      "     2240    2.540    0.001    2.583    0.001 /Users/mschochet/codeastro/orbitize/orbitize/lnlike.py:8(chi2_lnlike)\n",
      "     4480    1.895    0.000    3.388    0.001 /Users/mschochet/codeastro/orbitize/orbitize/kepler.py:18(tau_to_manom)\n",
      "     2240    1.851    0.001   23.750    0.011 /Users/mschochet/codeastro/orbitize/orbitize/sampler.py:292(prepare_samples)\n",
      "     4480    1.166    0.000    8.372    0.002 /Users/mschochet/anaconda3/envs/codeastro/lib/python3.10/site-packages/scipy/stats/_continuous_distns.py:10398(isf_right)\n",
      "    80660    1.011    0.000    1.011    0.000 {function Quantity.__array_ufunc__ at 0x115902710}\n",
      "    20187    0.878    0.000    0.878    0.000 {built-in method numpy.zeros}\n",
      "     2240    0.790    0.000   36.925    0.016 /Users/mschochet/codeastro/orbitize/orbitize/system.py:333(compute_all_orbits)\n",
      "     8960    0.747    0.000    0.763    0.000 /Users/mschochet/anaconda3/envs/codeastro/lib/python3.10/site-packages/scipy/special/_logsumexp.py:188(_sign)\n",
      "     4480    0.724    0.000    0.724    0.000 /Users/mschochet/codeastro/orbitize/orbitize/kepler.py:338(_mikkola_solver)\n",
      "     8960    0.620    0.000    0.620    0.000 /Users/mschochet/anaconda3/envs/codeastro/lib/python3.10/site-packages/scipy/stats/_continuous_distns.py:368(_norm_cdf)\n",
      "     2240    0.501    0.000   44.394    0.020 /Users/mschochet/codeastro/orbitize/orbitize/sampler.py:53(_logl)\n",
      "    45743    0.477    0.000    0.477    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
      "     2240    0.429    0.000   41.041    0.018 /Users/mschochet/codeastro/orbitize/orbitize/system.py:575(compute_model)\n",
      "     4480    0.415    0.000    1.035    0.000 /Users/mschochet/anaconda3/envs/codeastro/lib/python3.10/site-packages/scipy/stats/_continuous_distns.py:10215(mass_case_central)\n",
      "     8960    0.390    0.000    0.455    0.000 /Users/mschochet/codeastro/orbitize/orbitize/priors.py:464(draw_samples)\n",
      "   256708    0.367    0.000    0.381    0.000 {built-in method numpy.array}\n",
      "     2240    0.352    0.000   45.015    0.020 /Users/mschochet/codeastro/orbitize/orbitize/sampler.py:415(reject)\n",
      "73723/73693    0.322    0.000    1.087    0.000 /Users/mschochet/anaconda3/envs/codeastro/lib/python3.10/site-packages/astropy/units/core.py:2424(_expand_and_gather)\n",
      "     4480    0.311    0.000    0.311    0.000 /Users/mschochet/anaconda3/envs/codeastro/lib/python3.10/site-packages/scipy/stats/_continuous_distns.py:372(_norm_logcdf)\n",
      "     4480    0.302    0.000    0.302    0.000 /Users/mschochet/anaconda3/envs/codeastro/lib/python3.10/site-packages/scipy/special/_logsumexp.py:144(_wrap_radians)\n",
      "     2240    0.288    0.000    0.288    0.000 /Users/mschochet/codeastro/orbitize/orbitize/priors.py:517(transform_samples)\n",
      "        1    0.284    0.284    0.284    0.284 {method 'do_handshake' of '_ssl._SSLSocket' objects}\n",
      "     4480    0.272    0.000    9.336    0.002 /Users/mschochet/anaconda3/envs/codeastro/lib/python3.10/site-packages/scipy/stats/_distn_infrastructure.py:2317(isf)\n",
      "     1674    0.253    0.000    0.253    0.000 {method 'read' of '_io.BufferedReader' objects}\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x1060bd0f0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pstats\n",
    "\n",
    "mystats = pstats.Stats(\"profiler_output.txt\")\n",
    "\n",
    "mystats.sort_stats(\"time\").print_stats(30)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity Part 2: Investigate why `_logl` takes so long\n",
    "\n",
    "`_logl()` is a helper function in `orbitize!` to compute the log likelihood of the data given the model. The `_logl()` function itself has a short runtime but it is calling something that takes a long time that makes it have a long `cumtime`. We can use the `print_callees()` function to look at the stats of all the functions it calls.\n",
    "\n",
    "_Hint: We can see that `compute_model` is the function with the highest cumtime, but its tottime is low, so we know something it calls takes all the time. We must dig deeper! Keep digging down recursively to find what function called within `_logl()` that takes the longest._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mystats.print_callees(\"_logl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity Part 3: Which function calls `numpy.array` the most?\n",
    "\n",
    "`numpy.array` is a popular function because it gets called anytime a new numpy array gets created. We can use `print_callers()` to see which functions call it to look into potentially reducing the number of array creations to speed up the code. Which function in `orbitize` calls `numpy.array` the most times per function call (not including calls from subfunctions)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mystats.print_callers(\"numpy.array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Ways to Speed up Your Code\n",
    "\n",
    "If you identified the critical chunk of code to speed up, the general strategy is to remove unncessary computations as much as possible. There's unfortunately not one single method to speed up your code. But here are a few ideas:\n",
    "\n",
    "1. Eliminate computations that are not being used (e.g., computing variables you do not use; processing parts of images that will be discarded). \n",
    "2. Use `numpy` functions whenever possible for matrix operations\n",
    "3. Avoid using python `for` and `while` loops (see example below)\n",
    "4. If MKL/BLAS is being run inside of already-parallelized code, disable MKL/BLAS (see Day 1 parallelization materials for more details on MKL/BLAS).\n",
    "5. Reduce the number of iterations or increase the tolerance of routines that are unncessarily precise (e.g., optimizers can run for less iterations; sin and cos can be approximated by taylor expansions). \n",
    "6. Avoid copying variables when they do not need to be copied (e.g., if the input is already a numpy.array, you don't need to wrap it with `np.array()` to ensure it's a numpy array). \n",
    "7. Turn some of your python code into C-code and call it with Python\n",
    "\n",
    "Any other ideas?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here's an example of replacing a for loop with a faster masked operation.\n",
    "This is called VECTORIZATION.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# add 5 to each value in a list that's greater than 6\n",
    "arr = np.asarray([1, 4, 6, 7, 8, 10, 2])\n",
    "arr_2 = np.asarray([1, 4, 6, 7, 8, 10, 2])\n",
    "\n",
    "\n",
    "# first option (much slower for long arrays)\n",
    "for i, elem in enumerate(arr):\n",
    "    if elem > 6:\n",
    "        arr[i] += 5\n",
    "\n",
    "# second option (faster)\n",
    "mask_arr = arr_2 > 6\n",
    "arr_2[mask_arr] += 5\n",
    "\n",
    "# print resulting arrays for both cases\n",
    "print(arr)\n",
    "print(arr_2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Optimizing Strategy\n",
    "\n",
    "![](imgs/code_speedup.png)\n",
    "\n",
    "- Generally, it is important to focus your effort on speeding up the 1 or 2 slowest functions. If you speed up something that takes 50% of the runtime by a factor of 2, you speed up the code by a factor of 25%. If a piece of code takes 10% of the runtime, a factor of 2 improvement (which by itself is often hard to get) only gives you a 5% improvement in the end. **Focus your effort on optimizing the runtime of the one or two functions that take the most amount of time.**\n",
    "\n",
    "- For all else, it is generally more important to optimize code readibility over runtime. Code that is more readible is less prone to bugs and eaiser to maintain. If a piece of code takes only 0.0001% of the runtime, any amount of speed up is not worth making the code hard to read (from a person-hour perspective). Many times, it is difficult to gain more than a factor of 2 speed up in runtime. __Definitely think about the potential gain in speed-up versus how long it takes to write and maintian the code__.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra: Code Performance Extrapolation\n",
    "\n",
    "If your code takes 24 hours to run to completion, or needs to be run on a cluster, it is likely still easier to develop and benchmark it on your laptop. We can use extrapolation to estimate performance. \n",
    "\n",
    "### Runtime\n",
    "\n",
    "Generally algorithm runtimes scale as N^a where a is some positive number greater than 1, and N is the size of your data. (In CS terms, this is often disccused in \"Big O\" notation as  $O(N^a)$ runtime). If your data is too big or takes too long to benchmark on a single machine, try running your code on much smaller data. If you do this for ~3-5 different test data with different N, you can estimate what is the scaling of your code (i.e., what is \"a\"). Once you know the scaling you can use that scaling relation to extrapolate how long it will take on larger datasets. If you can also use more cores, you can divide the final runtime by the number of cores you can use. \n",
    "\n",
    "For example (very idealized), my code takes 1 second on 10x10 data, 100 seconds on 100x100 data, and 400 seconds on 200x200 data (all on a single core). My code scales as (N/10)^2 where N is the size of one dimension of the data. If I want to run it on 10000x10000 data with 100 cores, my code should take (1000^2)/100 = 10,000 seconds to run. In practice there will probably be some deviations to these estimates (due to overheads, CPU usage scheudling, other factors), but this scaling should give you a general sense of how long something will take. \n",
    "\n",
    "### Memory\n",
    "\n",
    "If you are worried that your program will run out of memory, try calculating how much memory your program uses. On modern 64-bit machines, a single number (an integer, float) takes up 8 bytes. If you have an array of 1000x1000 floats, then it will take up 8e6 bytes or 8 MBs. If you have 3-D numerical data with dimensions 1000x1000x1000, then it will take up 8e9 bytes or 8 GBs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "codeastro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
